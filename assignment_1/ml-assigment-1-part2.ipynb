{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emjAQ_34SozP",
    "tags": []
   },
   "source": [
    "# Programming Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKkKCHwsSozQ"
   },
   "source": [
    "In Part 2, you have to implement a logistic regression model and SVM model to predict the species of a pumpkin seeds using various morphological features.\n",
    "\n",
    "You have to write your code in this jupyter notebook and submit the solved jupyter notebook with the file name \\<Roll_No\\>_A1_2.ipynb for evaluation. You have to enter your code only in those cells which are marked as ```## CODE REQUIRED ##```, and you have to write your code only between ```### START CODE HERE ###``` and ```### END CODE HERE ###``` comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZ0CviUnZui0"
   },
   "source": [
    "## Part 2: Logistic Regression\n",
    "\n",
    "### Problem Statement\n",
    "Pumpkin seeds are frequently consumed as confection worldwide because of their adequate amount of protein, fat, carbohydrate, and mineral contents. There are two quality types of pumpkin seeds, ‘‘Urgup_Sivrisi’’ and ‘‘Cercevelik’’.Given various features of pumkin seeds such as area, perimeter, axis lengths etc. as input features, the task is to build a logistic regression model to predict the type of pumpkin seed.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "Dataset Filename: Pumpkin_Dataset.csv\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "+ Area: Represents the number of pixels within the boundaries of the pumpkin seed.\n",
    "\n",
    "+ Perimeter: Calculates the circumference by measuring the distance between pixels around the boundary of the pumpkin seed.\n",
    "\n",
    "+ Major Axis Length: The longest line that can be drawn on the pumpkin seed, representing the main axis distance.\n",
    "\n",
    "+ Minor Axis Length: The shortest line that can be drawn on the pumpkin seed, representing the minor axis distance.\n",
    "\n",
    "+ Convex Area: Returns the pixel count of the smallest convex shell that can contain the pumpkin seed.\n",
    "\n",
    "+ Equivalent Diameter: Diameter of a circle with the same area as the pumpkin seed.\n",
    "\n",
    "+ Eccentricity: This measures how round the ellipse, which has the same moments as the pumpkin seed has.\n",
    "\n",
    "+ Solidity: This is the ratio of the area of the pumpkin seed to the area of its convex hull. It measures the extent to which the shape is convex.\n",
    "\n",
    "+ Extent: Returns the ratio of the area of the pumpkin seed to the area of its bounding box.\n",
    "\n",
    "+ Roundness: Measure of how closely the shape of the pumpkin seed approaches that of a circle.\n",
    "\n",
    "+ Aspect Ratio: Ratio of the major axis length to the minor axis length.\n",
    "\n",
    "+ Compactness: Measure of the shape's compactness, which is the shape's deviation from being a perfect circle. In essence, compactness quantifies how efficiently an object's area is packed within its perimeter.\n",
    "\n",
    "Target Variable: Class: Çerçevelik and Ürgüp Sivrisi\n",
    "\n",
    "\n",
    "\n",
    "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
    "1. Reading the data\n",
    "2. Creating the sigmoid function\n",
    "2. Computing the loss function\n",
    "3. Computing the gradient of the loss\n",
    "4. Training the model using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T27XOBZrRZ4M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3luUk4L0WO_"
   },
   "source": [
    "### 2.1. Reading the data\n",
    "\n",
    "In the function ```load_data```, you have to read data from the file, store it in a dataframe and split the data from the dataframe into two numpy arrays X and y.\n",
    "\n",
    "**X** : data of the input features\n",
    "\n",
    "**y**  : data of the class labels\n",
    "\n",
    "The class labels in **y** should be replaced with `'0'(Çerçevelik)` and `'1'(Ürgüp Sivrisi)`, for corresponding classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-RV0JgEZi1h"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    This function loads the data into a pandas dataframe and converts it into X and y numpy arrays\n",
    "    y should be a binary numpy array with values 0 and 1, for 2 different classes\n",
    "    Args:\n",
    "        filepath: File path as a string\n",
    "    Returns:\n",
    "        X: Input data of the shape (# of samples, # of input features)\n",
    "        y: Target variable of the shape (# of sample,) with values 0 and 1, for 2 different classes\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    df=pd.read_csv(filepath)\n",
    "    \n",
    "    class_mapping = {'Çerçevelik': 0, 'Ürgüp Sivrisi': 1}\n",
    "    df['Class'] = df['Class'].map(class_mapping)\n",
    "    \n",
    "    # Splitting the data into X and y\n",
    "    X = df.drop('Class', axis=1).values  # All columns except Class\n",
    "    y = df['Class'].values  # Class column as target\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X,y\n",
    "\n",
    "filepath = None\n",
    "### START CODE HERE ###\n",
    "## set the file path\n",
    "filepath = \"data/Pumpkin_Dataset.csv\"\n",
    "### END CODE HERE ###\n",
    "\n",
    "X, y = load_data(filepath)\n",
    "\n",
    "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4u-qBe34wC9"
   },
   "source": [
    "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
    "\n",
    "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTO4etePa0i1"
   },
   "outputs": [],
   "source": [
    "## Data scaling and train-test split\n",
    "\n",
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_index = int(X.shape[0] * (1 - test_size))\n",
    "\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def min_max_scaler(X, feature_range=(0, 1)):\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "\n",
    "    X_scaled = (X-X_min)/(X_max-X_min)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "# Feature normalization\n",
    "X = min_max_scaler(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8SbmjD945Ra"
   },
   "source": [
    "### 2.2. Creating the Sigmoid Function\n",
    "Recall that for logistic regression, the model is represented as\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "\n",
    "The function below returns the value of the sigmoid function for an input numpy array z. If the numpy array 'z' stores multiple numbers, we'd like to apply the sigmoid function to each value in the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G44B1vITbBpE"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z: A scalar or numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g: sigmoid(z)\n",
    "    \"\"\"\n",
    "    g = None\n",
    "    z = z.astype(float)\n",
    "    ### START CODE HERE ###\n",
    "    z =np.clip(z, -500, 500)\n",
    "    g=1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV-Ygf1R8HMl"
   },
   "source": [
    "### 2.3. Computing the loss Function\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is -\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "Please complete the function loss_function that takes\n",
    "\n",
    " **X**  (input features)\n",
    "\n",
    " **y**  (class labels)\n",
    "\n",
    " **w**  (Parameters of the logistic regression model, (excluding the bias), a numpy array of the shape(1, number of features))\n",
    "\n",
    " **b**  (Bias value of the logistic regression model)\n",
    "\n",
    " You can use the Sigmoid function that you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9RVguhHbEVk"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def loss_function(X, y, w, b):\n",
    " \"\"\"\n",
    " Computes the loss function for all the training examples\n",
    " Args:\n",
    "        X: Input data of the shape (# of training samples, # of input features)\n",
    "        y: Target variable of the shape (# of training sample,)\n",
    "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
    "        b: Bias parameter (scalar) of the logistic regression model\n",
    "\n",
    "  Returns:\n",
    "        total_cost: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
    "\n",
    " \"\"\"\n",
    " m, n = X.shape\n",
    "\n",
    " total_cost = 0\n",
    " ### START CODE HERE ###\n",
    " z = np.dot(X,w.T) + b\n",
    " y_pred = sigmoid(z)\n",
    " \n",
    " epsilon = 1e-15\n",
    " y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    " \n",
    " total_cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    " ### END CODE HERE ###\n",
    "\n",
    " return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9YmUPnsASYE"
   },
   "source": [
    "### 2.4. Computing the Gradient of the Loss\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "In this step, you are required to complete the `compute_gradient_logistic_regression` function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
    "\n",
    "You can use the sigmoid function that you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWxbewKkbIB8"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def compute_gradient_logistic_regression(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient values of the loss function\n",
    "    Args:\n",
    "       X: Input data of the shape (# of training samples, # of input features)\n",
    "       y: Target variable of the shape (# of training sample,)\n",
    "       w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
    "       b: Bias parameter of the logistic regression model of the shape (1,1) or a scaler\n",
    "    Returns:\n",
    "       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
    "       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    z = np.dot(X,w.T) + b\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Compute gradients\n",
    "    dj_dw = (1/m) * np.dot((y_pred - y), X)\n",
    "    dj_db = (1/m) * np.sum(y_pred - y)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZMYX655MRFX"
   },
   "source": [
    "### 2.5. Training the model using Batch Gradient Descent\n",
    "\n",
    "Please complete the batch gradient descent algorithm for logistic regression to train and learn the parameters of the logistic regression model. You have to use ```loss_function``` and ```compute_gradient_logistic regression``` functions that you have implemented earlier in this assignment.\n",
    "\n",
    "In this ```batch_gradient_descent_logistic_regression``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
    "\n",
    "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "This function takes   ```X```    (input features),  ```y```  (class labels),  ```w_in```  (intial values of parameters(excluding bias)),  ```b_in```  (initial value for bias),  ```num_iters```   (number of iterations of training) as input.\n",
    "\n",
    "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOi07RVNbL_7"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def batch_gradient_descent_logistic_regression(X, y, w_in, b_in, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
    "    every 100 iterations\n",
    "\n",
    "    Args:\n",
    "        X: Input data of the shape (# of training samples, # of input features)\n",
    "        y: Target variable of the shape (# of training sample,)\n",
    "        w_in: Initial parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
    "        b_in: Initial bias parameter (scalar) of the logistic regression model\n",
    "        alpha: Learning rate\n",
    "        num_iters: number of iterations\n",
    "    Returns\n",
    "        w: Updated values of parameters of the model after training\n",
    "        b: Updated bias of the model after training\n",
    "        loss_hist: List of loss values for every iteration\n",
    "    \"\"\"\n",
    "\n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "\n",
    "    # list to store the loss values for every iterations\n",
    "    loss_hist = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    loss_hist = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Compute \n",
    "        current_loss = loss_function(X, y, w, b)\n",
    "        dj_db, dj_dw = compute_gradient_logistic_regression(X, y, w, b)\n",
    "        \n",
    "        # update parameters\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Store loss values\n",
    "        loss_hist.append([[current_loss]])\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Iteration {i+1}: Loss = {current_loss}')\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return w_in, b_in, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yeLdzT9-5Wc"
   },
   "source": [
    "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` is to be initialized as 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIBltyJh-Tjk"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    This function randomly initializes the model parameters (w and b)\n",
    "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
    "    alpha should be initialized as 0.01\n",
    "    Args:\n",
    "        None\n",
    "    Returns:\n",
    "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
    "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
    "        alpha: Learning rate\n",
    "    \"\"\"\n",
    "\n",
    "    initial_w = None\n",
    "    initial_b = None\n",
    "    alpha = None\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    n_features =12;\n",
    "    initial_w = np.random.randn(1, n_features)*0.01\n",
    "    initial_b = np.random.randn(1)*0.01\n",
    "    \n",
    "    alpha = 0.01\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return initial_w,initial_b,alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KT5CbQ8dZS_J"
   },
   "source": [
    "The following cell runs the batch gradient algorithm for\n",
    "```num_iterations=1000``` to train the logistic regression model. You can change the number of iterations to check any improvements in the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFTZi7pkbOdp"
   },
   "outputs": [],
   "source": [
    "# initialize the parameters (w an b) randomly\n",
    "initial_w, initial_b, alpha = initialize_parameters()\n",
    "num_iterations = 1000\n",
    "\n",
    "w, b, loss_hist = batch_gradient_descent_logistic_regression(X_train ,y_train, initial_w, initial_b, alpha, num_iterations)\n",
    "print(\"optimized parameter values w:\", w)\n",
    "print(\"optimized parameter value b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYP0-YUyY50x"
   },
   "source": [
    "### 2.6. Final Train and Test Accuracy\n",
    "After the logistic regression model is trained, we will predict the class labels for the training set and test set and we will compute the accuracy.\n",
    "\n",
    "Please complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n",
    "- First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example\n",
    "\n",
    "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
    "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic -\n",
    "\n",
    "  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n",
    "  \n",
    "  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZDUV2iwbQ_R"
   },
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "      \"\"\"\n",
    "      Predict whether the label is 0 or 1 using learned logistic regression parameters (w,b)\n",
    "\n",
    "      Args:\n",
    "        X: Input data of shape (number of sample, number of features)\n",
    "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
    "        b: Bias parameter of the logistic regression model\n",
    "\n",
    "      Returns:\n",
    "        p: Predictions for X using a threshold at 0.5\n",
    "      \"\"\"\n",
    "      m, n = X.shape\n",
    "      p = np.zeros(m)\n",
    "      ### START CODE HERE###\n",
    "      \n",
    "      z =np.dot(X,w.T) + b\n",
    "      y_pred = sigmoid(z)\n",
    "      \n",
    "      p = np.where(y_pred >= 0.5, 1, 0)\n",
    "\n",
    "\n",
    "      ### END CODE HERE ###\n",
    "      return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsrATkIy2KVy"
   },
   "source": [
    "Now let's use this to compute the accuracy on the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMmh6WdFbU9Y"
   },
   "outputs": [],
   "source": [
    "p_train = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
    "p_test = predict(X_test, w,b)\n",
    "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0VqjFy9ZK7D"
   },
   "source": [
    "Now, we plot the loss function values for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkmP3XHMTW74"
   },
   "outputs": [],
   "source": [
    "# PLotting the loss values for every training iterations\n",
    "\n",
    "loss_plot = [loss_hist[i][0][0] for i in range(len(loss_hist))]\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss function\")\n",
    "plt.plot(loss_plot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgKAU9RBEUlL"
   },
   "source": [
    "### 2.7 Experimenting with different values of the Hyperparemeters\n",
    "\n",
    "Previously we have chosen learning rate as 0.01. Now you have to train the model with learning rate as 0.01, 0.001 and 0.009 and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMcc3cuvIbD_"
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "\"\"\"\n",
    "Manually choose the hyperparameter (learning rate) and train the model.\n",
    "Then compare the performance with different value of hyperparameter.\n",
    "\"\"\"\n",
    "\n",
    "### START CODE HERE ###\n",
    "learning_rates = [0.01, 0.001, 0.009]\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    initial_w, initial_b, _ = initialize_parameters()\n",
    "    \n",
    "    w, b, loss_hist = batch_gradient_descent_logistic_regression(\n",
    "        X_train, y_train, initial_w, initial_b, lr, num_iters=1000\n",
    "    )\n",
    "    \n",
    "    # Accuracy Computation\n",
    "    train_preds = predict(X_train, w, b)\n",
    "    test_preds = predict(X_test, w, b)\n",
    "    train_acc = np.mean(train_preds == y_train) * 100\n",
    "    test_acc = np.mean(test_preds == y_test) * 100\n",
    "    \n",
    "    results.append({\n",
    "        'learning_rate': lr,\n",
    "        'train_accuracy': train_acc,\n",
    "        'test_accuracy': test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OwhwZF8Xa-x"
   },
   "source": [
    "## Task: Submitting Your Predictions\n",
    "\n",
    "After training your Logistic Regression model on the provided training dataset, you have generate predictions for the test dataset named `pumpkin_test_features.csv`, your final task is to save the predicted labels for the test dataset in a file named `RollNo_Logistic.csv`.\n",
    "\n",
    "### Instructions for Submission:\n",
    "1. **Format**:  \n",
    "   The `RollNo_Logistic.csv` file should contain one label per line, corresponding to the order of the test dataset features provided to you.Ensure there are no extra spaces, commas, or blank lines.\n",
    "\n",
    "2. **File Name**:  \n",
    "The file must be named `RollNo_Logistic.csv` exactly (case-sensitive).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-08T07:18:49.170939Z",
     "iopub.status.busy": "2025-01-08T07:18:49.170611Z",
     "iopub.status.idle": "2025-01-08T07:18:49.174584Z",
     "shell.execute_reply": "2025-01-08T07:18:49.173642Z",
     "shell.execute_reply.started": "2025-01-08T07:18:49.170915Z"
    },
    "id": "s0ZoZxbbXa-y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Write the code to save the predictions in required format\n",
    "\"\"\"\n",
    "### START CODE HERE ###\n",
    "def save_predictions(X_test_data, w, b, rollno):\n",
    "\n",
    "    test_data = pd.read_csv('data/pumpkin_test_features.csv')\n",
    "    X_test = min_max_scaler(test_data.values)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = predict(X_test, w, b)\n",
    "    \n",
    "    pd.DataFrame(predictions).to_csv(f'{rollno}_Logistic.csv', index=False, header=False)\n",
    "### END CODE HERE ###\n",
    "save_predictions(X_test, w, b, \"22CS30056\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d3VMS-6Xa-y"
   },
   "source": [
    "## Part 2: Support Vector Machine\n",
    "Just like implementing logistic regression now you have to implement SVM for the dataset\n",
    "\n",
    "**Note**: For SVM use of sklearn library is allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oS-wt69lXa-y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIi0py7dXa-y"
   },
   "source": [
    "### 2.1. Reading the data\n",
    "\n",
    "In the function ```load_data```, you have to read data from the file, store it in a dataframe and split the data from the dataframe into two numpy arrays X and y.\n",
    "\n",
    "**X** : data of the input features\n",
    "\n",
    "**y**  : data of the class labels\n",
    "\n",
    "The class labels in **y** should be replaced with '0'(Çerçevelik) and '1'(Ürgüp Sivrisi), for corresponding classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wT6NWT1Xa-y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## CODE REQUIRED ##\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    This function loads the data into a pandas dataframe and converts it into X and y numpy arrays\n",
    "    y should be a binary numpy array with values 0 and 1, for 2 different classes\n",
    "    Args:\n",
    "        filepath: File path as a string\n",
    "    Returns:\n",
    "        X: Input data of the shape (# of samples, # of input features)\n",
    "        y: Target variable of the shape (# of sample,) with values 0 and 1, for 2 different classes\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    class_mapping = {'Çerçevelik': 0, 'Ürgüp Sivrisi': 1}\n",
    "    df['Class'] = df['Class'].map(class_mapping)\n",
    "    \n",
    "    X = df.drop('Class', axis=1).values\n",
    "    y = df['Class'].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "filepath = \"Pumpkin_Dataset.csv\"\n",
    "X, y = load_data(filepath)\n",
    "print(\"Shape of X: \", X.shape, \"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFVGdnV4Xa-z"
   },
   "source": [
    "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
    "\n",
    "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQTD86ctXa-z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Data scaling and train-test split\n",
    "\n",
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_index = int(X.shape[0] * (1 - test_size))\n",
    "\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def min_max_scaler(X, feature_range=(0, 1)):\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "\n",
    "    X_scaled = (X-X_min)/(X_max-X_min)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "# Feature normalization\n",
    "X = min_max_scaler(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASBPCeSqXa-z"
   },
   "source": [
    "### 2.2. Initializing SVM and training the model\n",
    "Initialize the SVM classifier using the `SVC` class from `sklearn`. You can experiment with different kernel types such as `linear`, `poly`, `rbf`, etc.\n",
    "\n",
    "Fit the SVM model using the training dataset (`X_train`, `y_train`) to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sbw1uJI1Xa-z",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1o7_sIXHXa-z"
   },
   "source": [
    "### 2.3. Evaluate the model on both training and testing data\n",
    "Evaluate your model and interpret your results based on the performance\n",
    "\n",
    "Compare `SVM` Model with `Logistic Regression`, which one performed better, state the possible reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut9yDZY6Xa-7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute the accuracy of SVM model both test and training dataset.\n",
    "\"\"\"\n",
    "# Make predictions\n",
    "train_predictions = svm_model.predict(X_train)\n",
    "test_predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracies\n",
    "train_accuracy = accuracy_score(y_train, train_predictions) * 100\n",
    "test_accuracy = accuracy_score(y_test, test_predictions) * 100\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ofebiYHXa-7"
   },
   "source": [
    "## Task: Submitting Your Predictions\n",
    "\n",
    "After training your Logistic Regression model on the provided training dataset, you have generate predictions for the test dataset, your final task is to save the predicted labels for the test dataset in a file named `RollNo_SVM.csv`.\n",
    "\n",
    "### Instructions for Submission:\n",
    "1. **Format**:  \n",
    "   The `RollNo_SVM.csv` file should contain one label per line, corresponding to the order of the test dataset features provided to you.Ensure there are no extra spaces, commas, or blank lines.\n",
    "\n",
    "2. **File Name**:  \n",
    "The file must be named `RollNo_SVM.csv` exactly (case-sensitive).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTchQnwmXa-8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load and preprocess test data\n",
    "test_data = pd.read_csv('pumpkin_test_features.csv')\n",
    "X_test = min_max_scaler(test_data.values)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = svm_model.predict(X_test)\n",
    "\n",
    "# Save to CSV\n",
    "pd.DataFrame(predictions).to_csv('22CS30056_SVM.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
