{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emjAQ_34SozP",
        "tags": []
      },
      "source": [
        "# Programming Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkKCHwsSozQ"
      },
      "source": [
        "## Random Forest Implementation for Classification Task\n",
        "\n",
        "### Instructions\n",
        "\n",
        "1. **Objective**:  \n",
        "   Implement a Random Forest classifier for Classification Task in this notebook.\n",
        "\n",
        "2. **File Naming and Submission**:  \n",
        "   - Save the completed notebook as `<Roll_No>_P1.ipynb`, replacing `<Roll_No>` with your roll number.  \n",
        "   - Ensure all code, outputs, and results are included in the submitted file.\n",
        "\n",
        "3. **Code Placement**:  \n",
        "   - Write your code **only** in the cells marked as `## CODE REQUIRED ##`.  \n",
        "   - Place your implementation between the following comments:  \n",
        "     ```python\n",
        "     ### START CODE HERE ###\n",
        "     # Your code along with comments goes here\n",
        "     ### END CODE HERE ###\n",
        "     ```\n",
        "   - Do not modify any other sections of the notebook.\n",
        "  \n",
        "\n",
        "4. **Execution and Output**:  \n",
        "   - Run all cells to ensure the code executes correctly without errors.  \n",
        "   - Save the notebook **with all outputs visible** before submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPf8pHpSozR"
      },
      "source": [
        "## Random Forest Classifier\n",
        "\n",
        "### Dataset Details\n",
        "\n",
        "The dataset being used is the **Indians Diabetes Database**. This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
        "\n",
        "#### Attributes:\n",
        "1. **Pregnancies**: Number of times pregnant\n",
        "2. **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
        "3. **BloodPressure**: Diastolic blood pressure (mm Hg)\n",
        "4. **SkinThickness**: Triceps skin fold thickness (mm)\n",
        "5. **Insulin**: 2-Hour serum insulin (mu U/ml)\n",
        "6. **BMI**: Body mass index (weight in kg/(height in m)^2)\n",
        "7. **DiabetesPedigreeFunction**: Diabetes pedigree function\n",
        "8. **Age**: Age (years)\n",
        "9. **Outcome** (class attribute, Target Variable): This is the target variable with the following categories:\n",
        "   - 0: No diabetes\n",
        "   - 1: Diabetes\n",
        "\n",
        "You are required to use the exact CSV files (**dataset_train.csv and dataset_test.csv**) provided with this assignment for all analyses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import necessary library functions"
      ],
      "metadata": {
        "id": "AST2vAKPIpTv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJDNL8y8SozR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import sqrt\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility.\n",
        "# DO NOT modify this section, as it is essential for automated evaluation.\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)"
      ],
      "metadata": {
        "id": "lOm8yWzpI4MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgnxuEDkSozS"
      },
      "source": [
        "# Instructions: Loading and Processing the Dataset\n",
        "\n",
        "Follow these steps to load and process the dataset in Python:\n",
        "\n",
        "\n",
        "### **Steps Performed in the Function**\n",
        "\n",
        "- Step 1: Load the Dataset: Use the `pandas` library to load the dataset from the specified CSV file (`file_name`).\n",
        "\n",
        "\n",
        "- Step 2: Separate Features and Target Variable: Split the dataset into:\n",
        "  - **Features (`X`)**: A 2D array containing all columns except the target column\n",
        "  - **Target (`y`)**: A 1D array containing the target (class label) column (0/1)\n",
        "\n",
        "- Step 3: Return Processed Data: The function returns:\n",
        "\n",
        "1. `X` should be of type `<class 'numpy.ndarray'>` and have shape `(num_samples, num_features)`.\n",
        "2. `y` should be of type `<class 'numpy.ndarray'>` and have shape `(num_samples,)`.\n",
        "3. `df` should be of type `<pandas.DataFrame>`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEal5SFmSozS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "### CODE REQUIRED ###\n",
        "def load_data(file_name):\n",
        "    \"\"\"\n",
        "    This function loads the dataset from a CSV file and returns the features, target labels and dataframe\n",
        "\n",
        "    Returns:\n",
        "        X (ndarray): A 2D array of normalized features.\n",
        "        y (ndarray): A 1D array of target values (class labels).\n",
        "        df(pandas.DataFrame): The original DataFrame containing the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "   ### START CODE HERE ###\n",
        "   ### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, df_train = load_data('dataset_train.csv')"
      ],
      "metadata": {
        "id": "hpMrHYoJKKnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset loaded: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
        "feature_names = df_train.columns[:-1]\n",
        "print(f\"Feature names: {', '.join(feature_names)}\")"
      ],
      "metadata": {
        "id": "gdI4gDyPAiMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=df_train, x='Outcome')\n",
        "plt.title('Distribution of Output Labels')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x4EL4CPMK0ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 8  # Updating the number of features\n",
        "num_cols = 3\n",
        "num_rows = (num_features + num_cols - 1) // num_cols  # Adjusting row count\n",
        "\n",
        "plt.figure(figsize=(15, 4 * num_rows))\n",
        "\n",
        "for i, feature in enumerate(feature_names):\n",
        "    plt.subplot(num_rows, num_cols, i + 1)\n",
        "    sns.boxplot(data=df_train, x='Outcome', y=feature)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.title(f'{feature} Distribution by Output Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gu0gX4DiLAbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation heatmap\n",
        "# This heatmap shows the correlation between different features in the dataset.\n",
        "# It helps us understand how strongly the features are related to each other, with positive or negative correlations.\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df_train.drop('Outcome', axis=1).corr(), annot=True, cmap='coolwarm', center=0)\n",
        "plt.title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KRKwKjw-Mi0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLmD1I3-SozT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    \"\"\"\n",
        "    This class represents a single node in a decision tree.\n",
        "    It can either be a decision node (with a feature and threshold) or a leaf node (with a value).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        \"\"\"\n",
        "        Initializes a new Node. It can represent either a decision node or a leaf node.\n",
        "\n",
        "        Parameters:\n",
        "            feature (int or None): The feature index for splitting at this node (None for leaf nodes).\n",
        "            threshold (float or None): The threshold value for splitting (None for leaf nodes).\n",
        "            left (Node or None): The left child node (None for leaf nodes).\n",
        "            right (Node or None): The right child node (None for leaf nodes).\n",
        "            value (int or None): The class label for leaf nodes, None for decision nodes.\n",
        "        \"\"\"\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "    def is_leaf(self):\n",
        "        \"\"\"\n",
        "        Determines whether the current node is a leaf node.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the node is a leaf (i.e., has a value), False otherwise.\n",
        "        \"\"\"\n",
        "        return self.value is not None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-by-Step Explanation for Decision Tree Ckass\n",
        "\n",
        "### 1.**_grow_tree**:\n",
        "This function is responsible for recursively growing a decision tree for classification using a custom Random Forest implementation. The tree-building process involves splitting the data based on the best feature and threshold at each step, using a metric called information gain.\n",
        "\n",
        "##### a. **Input Parameters:**\n",
        "- `X` : Feature matrix (input data)\n",
        "- `y` : Labels (target values)\n",
        "- `depth` : The current depth of the tree (used for stopping conditions)\n",
        "\n",
        "##### b. **Stopping Criteria:**\n",
        "   The recursion stops if any of the following conditions are met:\n",
        "   - The current depth of the tree reaches the `self.max_depth`.\n",
        "   - The number of samples in a node is less than `self.min_samples_split`.\n",
        "   - There is only one unique label in the current node (that is all samples in X have the same label).\n",
        "   \n",
        "   If any of these criteria are met, a leaf node is created with the most common label from `y`.\n",
        "\n",
        "##### c. **Feature Selection:**\n",
        "   - Randomly select a subset of features (`feat_idxs`) from the total number of features.\n",
        "   - This subset size is determined by `self.num_features` (the number of features used for splitting).\n",
        "\n",
        "##### d. **Finding the Best Split:**\n",
        "   - For each feature in the selected subset (`feat_idx`), do the following:\n",
        "     - Get the values in the feature column (`X_column`).\n",
        "     - Find all possible thresholds (unique values) for this feature.\n",
        "     - For each threshold:\n",
        "       - Split the data into two groups: left (`left_idxs`) and right (`right_idxs`), based on whether the feature value is less than the threshold or not.\n",
        "       - If both the left and right groups have at least one sample, calculate the **information gain** by splitting the dataset at this threshold.\n",
        "       - Keep track of the best threshold that gives the highest information gain.\n",
        "    \n",
        "\n",
        "##### e. **Create Leaf Node (if no valid split):**\n",
        "   If no valid split is found (i.e., no feature gives a positive information gain), create a leaf node with the most common label in the target labels (`y`).\n",
        "\n",
        "##### f. **Recursively Grow the Left and Right Subtrees:**\n",
        "   - Recursively call the `_grow_tree` function to grow the left and right subtrees using the split data.\n",
        "   - The left subtree is created using the data that satisfies `X_column < threshold`, and the right subtree is created using the data where `X_column >= threshold`.\n",
        "   \n",
        "   left = self._grow_tree(\n",
        "            left_X,\n",
        "            left_y,\n",
        "            depth + 1\n",
        "        )\n",
        "\n",
        "##### g. **Return the Node:**\n",
        "   - Once the left and right subtrees are created, return a node that represents the decision at this step in the tree, which contains:\n",
        "     - The feature index used for the split (`feature`).\n",
        "     - The threshold used for the split (`threshold`).\n",
        "     - The left child subtree (type Node same as return type of _grow_tree) (`left`).\n",
        "     - The right child subtree (type Node) (`right`).\n",
        "\n",
        "### 2. **fit**:\n",
        "\n",
        "a. **Initialize `self.num_classes`**:  \n",
        "   Set `self.num_classes` to the number of unique classes present in `y`.\n",
        "\n",
        "b. **Grow the Tree**:  \n",
        "   Call the `grow_tree` method with `X` and `y` as inputs. Assign the result to `self.root`.\n",
        "\n",
        "### 3. **_information_gain**:\n",
        "\n",
        "a. **Calculate Weights for Left and Right Child Nodes**:  \n",
        "   Compute the weight for the left child as:  \n",
        "   $$\n",
        "   \\text{weight}_l = \\frac{\\text{len(left_child)}}{\\text{len(parent)}}\n",
        "   $$  \n",
        "   Compute the weight for the right child as:  \n",
        "   $$\n",
        "   \\text{weight}_r = \\frac{\\text{len(right_child)}}{\\text{len(parent)}}\n",
        "   $$\n",
        "\n",
        "b. **Calculate Information Gain due to split**:  \n",
        "   Compute the Gini impurity of the parent node and subtract the weighted Gini impurities of the left and right child nodes:  \n",
        "   $$\n",
        "   \\text{gain} = \\text{Gini(parent)} - \\left( \\text{weight}_l \\cdot \\text{Gini(left_child)} + \\text{weight}_r \\cdot \\text{Gini(right_child)} \\right)\n",
        "   $$\n",
        "\n",
        "### 4. **_gini**:\n",
        "\n",
        "a. **Compute Label Counts**:  \n",
        "   Compute the unique labels and their counts in \\( y \\)\n",
        "\n",
        "b. **Calculate Probabilities**:  \n",
        "   Compute the probabilities for each unique label:\n",
        "   $$\n",
        "   \\text{probabilities} = \\frac{\\text{counts}}{\\text{len}(y)}\n",
        "   $$\n",
        "\n",
        "c. **Compute Gini Impurity**:  \n",
        "   Compute the Gini impurity as:\n",
        "   $$\n",
        "   \\text{Gini}(y) = 1 - \\sum \\left( \\text{probabilities}^2 \\right)\n",
        "   $$\n",
        "\n",
        "### **5. predict**:\n",
        "\n",
        "\n",
        "a. **Predict Class Labels for Input Samples**:  \n",
        "   For each input sample \\( x \\) in \\( X \\), traverse the tree to make a prediction using the `_traverse_tree` function. Return the predictions as a NumPy array.\n",
        "\n",
        "### **6. _traverse_tree**:\n",
        "\n",
        "a. **Check if the Node is a Leaf**:  \n",
        "   If the current node is a leaf, return the value of the node.\n",
        "\n",
        "b. **Traverse the Left or Right Subtree**:  \n",
        "   traverse the left or right child based on the threshold value.\n",
        "   "
      ],
      "metadata": {
        "id": "lRcwxnhRxUpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CODE REQUIRED ###\n",
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    This class implements a decision tree classifier.\n",
        "    It supports growing a tree with specified max depth, minimum samples for splitting,\n",
        "    and features to consider at each split.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_depth=None, min_samples_split=None, num_features=None):\n",
        "        \"\"\"\n",
        "        Initializes the decision tree with user-defined parameters.\n",
        "\n",
        "        Parameters:\n",
        "            max_depth (int): Maximum depth of the tree.\n",
        "            min_samples_split (int): Minimum samples required to split a node.\n",
        "            num_features (int or None): Number of features to consider when making a split.\n",
        "        \"\"\"\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.num_features = num_features\n",
        "        self.root = None\n",
        "\n",
        "    def visualize_tree(self, node=None, depth=0, feature_names=None, prefix=\"\"):\n",
        "        \"\"\"\n",
        "        Recursively visualizes the tree in a human-readable format with proper left/right indentation.\n",
        "\n",
        "        Parameters:\n",
        "            node (Node): The current node in the tree.\n",
        "            depth (int): The current depth in the tree.\n",
        "            feature_names (list): List of feature names to display instead of indices.\n",
        "            prefix (str): Current line prefix for drawing the tree structure.\n",
        "\n",
        "        Returns:\n",
        "            str: A string representation of the tree.\n",
        "        \"\"\"\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        output = \"\"\n",
        "\n",
        "        if node.is_leaf():\n",
        "            return f\"{prefix}└── [Prediction: {node.value}]\\n\"\n",
        "\n",
        "        feature_name = f\"Feature {node.feature}\" if feature_names is None else feature_names[node.feature]\n",
        "        output += f\"{prefix}└── {feature_name} < {node.threshold:.2f}\\n\"\n",
        "\n",
        "        # Left subtree (add indentation and connection lines)\n",
        "        left_prefix = prefix + \"    │   \"\n",
        "        output += self.visualize_tree(node.left, depth + 1, feature_names, left_prefix)\n",
        "\n",
        "        # Right subtree (add indentation and connection lines)\n",
        "        right_prefix = prefix + \"    │   \"\n",
        "        output += f\"{prefix}└── {feature_name} >= {node.threshold:.2f}\\n\"\n",
        "        output += self.visualize_tree(node.right, depth + 1, feature_names, right_prefix)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        \"\"\"\n",
        "        Recursively grows the decision tree.\n",
        "\n",
        "        Parameters:\n",
        "            X (ndarray): Input feature matrix.\n",
        "            y (ndarray): Target labels.\n",
        "            depth (int): Current depth in the tree.\n",
        "\n",
        "        Returns:\n",
        "            Node: The root node of the subtree.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits the decision tree to the data by growing the tree recursively.\n",
        "\n",
        "        Parameters:\n",
        "            X (ndarray): Input feature matrix.\n",
        "            y (ndarray): Target labels.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def _information_gain(self, parent, left_child, right_child):\n",
        "        \"\"\"\n",
        "        Calculates the information gain based on Gini impurity.\n",
        "\n",
        "        Parameters:\n",
        "            parent (ndarray): The labels of the parent node.\n",
        "            left_child (ndarray): The labels of the left child node.\n",
        "            right_child (ndarray): The labels of the right child node.\n",
        "\n",
        "        Returns:\n",
        "            float: The information gain.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    def _gini(self, y):\n",
        "        \"\"\"\n",
        "        Calculates the Gini impurity for a given set of labels.\n",
        "\n",
        "        Parameters:\n",
        "            y (ndarray): The labels for the node.\n",
        "\n",
        "        Returns:\n",
        "            float: The Gini impurity score.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the class labels for a given input.\n",
        "\n",
        "        Parameters:\n",
        "            X (ndarray): Input feature matrix.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: Predicted class labels.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    def _traverse_tree(self, x, node):\n",
        "        \"\"\"\n",
        "        Traverses the tree to make a prediction for a single sample.\n",
        "\n",
        "        Parameters:\n",
        "            x (ndarray): The input feature vector.\n",
        "            node (Node): The current node being visited.\n",
        "\n",
        "        Returns:\n",
        "            int: The predicted class label.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n"
      ],
      "metadata": {
        "id": "tYm2ciqiP22s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForest Class Implementation\n",
        "\n",
        "In this section, we describe the functionality of the `RandomForest` class, which implements a basic version of the Random Forest algorithm. The key features of this class include its initialization, fitting, and prediction capabilities.\n",
        "\n",
        "\n",
        "#### 1. Training (`fit` method)\n",
        "The `fit` method is responsible for training the Random Forest model. It begins by initializing an empty list `self.trees` to store the trained decision trees.\n",
        "\n",
        "The method then loops over the number of trees, creating and training a decision tree on a different random sample of the data each time. This process is called **Bootstrap Sampling**, where each tree is trained on a randomly selected subset of the data with replacement. This helps introduce diversity among the trees, which is crucial for the performance of the Random Forest.\n",
        "\n",
        "For each iteration:\n",
        "- A new `DecisionTree` object is instantiated, and trained on the bootstrapped data.\n",
        "\n",
        "#### 2. Prediction (`predict` method)\n",
        "Once the forest is trained, the `predict` method is used to make predictions on new data:\n",
        "- The predictions are obtained by passing the input data (`X`) through all the decision trees in the forest. Each tree makes a prediction, and these predictions are collected into an array.\n",
        "- To obtain the final prediction for each sample, the method takes a **majority vote** across the individual tree predictions. The class label that appears most frequently is selected as the final prediction for that sample.\n",
        "\n",
        "#### 3. The `predict_probability` Function\n",
        "\n",
        "The `predict_probability` function takes an input matrix `X` (feature data) and predicts the probability that each sample belongs to class 1 based on the majority vote of the trees in the Random Forest.\n",
        "\n",
        "- **Input**: `X` (ndarray) – Feature matrix with samples and their corresponding features.\n",
        "- **Output**: (ndarray) – Probability estimates for each sample belonging to class 1.\n",
        "- **What it does**: The function calculates the proportion of trees that predict class 1 for each sample and returns these proportions as the probability of class 1."
      ],
      "metadata": {
        "id": "0Ng0krk6YR5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CODE REQUIRED ###\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=None, max_depth=None, min_samples_split=None,\n",
        "                 num_features=None):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.num_features = num_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    def predict_probability(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the probability of class 1 for the given input.\n",
        "        \"\"\"\n",
        "\n",
        "        ### START CODE HERE ###\n",
        "\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    def visualize_trees(self, feature_names=None, max_trees=1):\n",
        "        \"\"\"Visualize the first few trees in the forest\"\"\"\n",
        "        # Determine the number of trees to visualize (limit by max_trees or total number of trees)\n",
        "        trees_to_show = min(max_trees, len(self.trees))\n",
        "\n",
        "        # Loop to display each tree in the forest\n",
        "        for i in range(trees_to_show):\n",
        "            print(f\"\\nTree {i+1}:\")\n",
        "            print(self.trees[i].visualize_tree(feature_names=feature_names))  # Visualize each tree\n"
      ],
      "metadata": {
        "id": "GRuWcd699Lvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Random Forest Model\n",
        "\n",
        "1. **Train the Model**:\n",
        "   - Instantiate the Random Forest model\n",
        "\n",
        "2. **Print the Train Accuracy**:\n",
        "   - After training the model, print the training accuracy to evaluate the model's performance on the training data along with confusion matrix.\n",
        "\n",
        "3. **Visualize the Trees** (Optional):\n",
        "   - You can visualize one or more of the trees from the trained Random Forest model using the `visualize_trees` function."
      ],
      "metadata": {
        "id": "fZP-Rgh-h5ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion(predictions, y_test, target_names=None):\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=target_names,\n",
        "                yticklabels=target_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return\n"
      ],
      "metadata": {
        "id": "uLaP66j7jKmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, X_test, y_test, target_names=None):\n",
        "    \"\"\"\n",
        "    Test a trained Random Forest model on the given test data.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained Random Forest model.\n",
        "    - X_test: Test features (2D array).\n",
        "    - y_test: Test labels (1D array).\n",
        "    - feature_names: List of feature names (optional, for feature importance plotting).\n",
        "    - target_names: List of target class names (optional, for confusion matrix plotting).\n",
        "\n",
        "    Returns:\n",
        "    - predictions: Predicted labels for the test data.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Predict on the test data\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = np.mean(predictions == y_test) * 100\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(f\"Evaluation completed in {time.time() - start_time:.2f} seconds\")\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "bMOdrU2hiSwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Assignment Instructions\n",
        "\n",
        "## Task Overview:\n",
        "Each student will use a specific set of hyperparameters for training the Random Forest model, determined by the remainder when dividing their roll number by 2. Follow the instructions below carefully to ensure you are using the correct hyperparameters.\n",
        "\n",
        "### Steps to Select Hyperparameters:\n",
        "\n",
        "1. **Identify Your Roll Number**:  \n",
        "   Use the last two digits of your roll number to determine the corresponding set of hyperparameters.  \n",
        "   For example, if your roll number is **20CS30065**, you will use the last two digits (65), and calculate the remainder when divided by 2.\n",
        "\n",
        "2. **Calculate the Remainder**:  \n",
        "   Take the last two digits of your roll number and find the remainder when divided by 2. This will determine which hyperparameter set you should use:\n",
        "   - **If the remainder is 0** (e.g., roll number 30060), use **Hyperparameter Set 0**.\n",
        "   - **If the remainder is 1** (e.g., roll number 30061), use **Hyperparameter Set 1**.\n",
        "   \n",
        "3. **Hyperparameter Sets**:  \n",
        "   Here are the two sets of hyperparameters:\n",
        "\n",
        "   ### Hyperparameter Set 0 (For Roll Numbers % 2 == 0):\n",
        "   - `num_features = 7`\n",
        "   - `n_trees = 11`\n",
        "   - `max_depth = 5`\n",
        "   - `min_samples_split = 2`\n",
        "\n",
        "   ### Hyperparameter Set 1 (For Roll Numbers % 2 == 1):\n",
        "   - `num_features = 8`\n",
        "   - `n_trees = 9`\n",
        "   - `max_depth = 7`\n",
        "   - `min_samples_split = 2`\n",
        "\n",
        "\n",
        "\n",
        "4. **Failure to Follow Instructions**:\n",
        "   - Ensure you follow the instructions carefully. Incorrectly selecting or applying the wrong set of hyperparameters will result in **penalization of marks**.\n",
        "\n"
      ],
      "metadata": {
        "id": "3kXxerTDW5TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CODE REQUIRED ###\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "'''DEFINE A RANDOM FOREST AND FIT TRAINING DATASET'''\n",
        "\n",
        "num_features =\n",
        "n_trees =\n",
        "max_depth =\n",
        "min_samples_split =\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n"
      ],
      "metadata": {
        "id": "p2rLmsWh86Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model.visualize_trees(feature_names=feature_names)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qDnDSbrwgNCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = test_model(rf_model, X_train, y_train,[0,1])"
      ],
      "metadata": {
        "id": "2bkFTIi889zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion(predictions, y_train, [0,1])"
      ],
      "metadata": {
        "id": "EaFeP_ghjdtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_roc_curve(model, X_test, y_test):\n",
        "    y_scores = model.predict_probability(X_test)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve)')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_roc_curve(rf_model, X_train, y_train)"
      ],
      "metadata": {
        "id": "qssppFO4s1Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S34KA2LlXXbR"
      },
      "source": [
        "# Submission Instructions\n",
        "\n",
        "## Task Overview:\n",
        "The goal is to train a Random Forest model using the provided training dataset and generate predictions for the `dataset_test.csv` dataset. The predictions should be saved in a file named `RollNo_P1.csv`. Additionally, you need to submit the model's predicted probabilities on `X_test` as `RollNo_test_prob.npy`. The ROC curve will be evaluated automatically.\n",
        "\n",
        "Do **not** modify the order or any columns from the `dataset_test.csv`.\n",
        "\n",
        "1. **Generate Predicted Probabilities**:\n",
        "   - Use the model to generate the predicted probabilities for `X_test`.\n",
        "   - Save these probabilities in a file named `RollNo_test_prob.npy`:\n",
        "     ```python\n",
        "     test_predict_probability = rf_model.predict_proba(X_test)\n",
        "     np.save('RollNo_test_prob.npy', test_predict_probability)\n",
        "     ```\n",
        "2. **Submit Your Files**:\n",
        "   - Submit both files:\n",
        "     - `RollNo_P1.csv` (containing the predictions)\n",
        "     - `RollNo_test_prob.npy` (containing the predicted probabilities)\n",
        "\n",
        "### Important Notes:\n",
        "- Failing to follow the naming conventions or modifying the structure of the test dataset will result in a penalty of marks.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}